{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs) and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MNISTtools import load, show\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ltrain = load(dataset='training', path='../dataset/')\n",
    "xtest, ltest = load(dataset='testing', path='../dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain.shape = (784, 60000)\n",
      "ltrain.shape = (60000,)\n",
      "size of training set = 60000\n",
      "feature dimension = 10\n"
     ]
    }
   ],
   "source": [
    "print(f'xtrain.shape = {xtrain.shape}')\n",
    "print(f'ltrain.shape = {ltrain.shape}')\n",
    "print(f'size of training set = {ltrain.size}')\n",
    "print(f'feature dimension = {ltrain.max()+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display the image of index 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label of index 42 = 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'label of index 42 = {ltrain[42]}')\n",
    "show(xtrain[:,42].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The range and type of `xtrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of xtrain = from 0 to 255\n",
      "type of xtrain = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(f'range of xtrain = from {xtrain.min()} to {xtrain.max()}')\n",
    "print(f'type of xtrain = {type(xtrain)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize `xtrain`\n",
    "Normalize `xtrain` s.t. it is in the range `[-1,1]` of type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "    '''\n",
    "    x = x.astype(np.float32)\n",
    "    #x = (x-128)/128\n",
    "    x = x*2/255-1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min normalized xtrain = -1.0078431367874146\n",
      "max normalized xtrain = -0.9921568632125854\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print(f'min normalized xtrain = {normalize_MNIST_images(xtrain).min()}')\n",
    "print(f'max normalized xtrain = {normalize_MNIST_images(xtrain).max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    '''\n",
    "    Convert label (n,) to onehot form (d,n).\n",
    "    '''\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtrain shape = (10, 60000)\n",
      "(array([7]),)\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(f'dtrain shape = {dtrain.shape}')\n",
    "print(f'{np.where(dtrain[:,42] == 1)}')\n",
    "print(f'{np.where(dtrain[:,42] == 1) == ltrain[42]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. One-hot decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    '''\n",
    "    Inputs:\n",
    "        d: one-hot encoding labels\n",
    "    '''\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(ltrain == onehot2label(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    '''\n",
    "    Inputs: \n",
    "        a: data\n",
    "    '''\n",
    "    exp_a = np.exp(a - np.max(a, axis=0))\n",
    "    return exp_a / np.sum(exp_a, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. $$ \n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_i} = {g(\\boldsymbol{a})}_i (1-{g(\\boldsymbol{a})_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_i} & = \n",
    "  \\frac{\\partial}{\\partial a_i}\\frac{e^{a_i}}{\\Sigma_{k=1}^{N} e^{a_k}} \\\\\n",
    "& = \\frac{e^{a_i}\\Sigma_{k=1}^{N}e^{a_k}-e^{a_i}e^{a_i}}{(\\Sigma_{k=1}^{N}e^{a_k})^2} \\\\\n",
    "& = \\frac{e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\frac{\\Sigma_{k=1}^{N}e^{a_k}-e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\\\\n",
    "& = {g(\\boldsymbol{a})}_i (1-{g(\\boldsymbol{a})_i})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9$$\n",
    "\\frac{\\partial{g(\\boldsymbol{a})_i}}{\\partial a_j}=-g(\\boldsymbol{a})_i g(\\boldsymbol{a})_j \\quad for \\, j \\neq i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_j} & = \n",
    "  \\frac{\\partial}{\\partial a_j} \\frac{e^{a_i}}{\\Sigma_{k=1}^{N} e^{a_k}} \\\\\n",
    "& = \\frac{0-e^{a_i}e^{a_j}}{(\\Sigma_{k=1}^{N}e^{a_k})^2} \\\\\n",
    "& = -\\frac{e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\frac{e^{a_j}}{\\Sigma_{k=1}^{N}e^{a_k}} \\\\\n",
    "& = -{g(\\boldsymbol{a})}_i g(\\boldsymbol{a})_j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute the $\\delta$\n",
    "$$\n",
    "\\delta = g(a)\\otimes e-<g(a),e>g(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    '''\n",
    "    Compute delta for the backpropagation.\n",
    "    \n",
    "    Inputs:\n",
    "        a: predictions before softmax\n",
    "        e: onehot labels\n",
    "        \n",
    "    Returns:\n",
    "        delta\n",
    "    '''\n",
    "    softmax_a = softmax(a)\n",
    "    \n",
    "    # element-wise product\n",
    "    e_prod = softmax_a*e\n",
    "    \n",
    "    # dot product\n",
    "    dot_prod = np.sum(e_prod, axis=0)\n",
    "    \n",
    "    return e_prod - dot_prod*softmax_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Numerical approximations for $\\delta\\$\n",
    "$$\n",
    "\\delta=\\frac{\\partial g(a)}{\\partial a}\\times e = \\lim_{\\epsilon \\to 0}\\frac{g(a+\\epsilon e)-g(a)}{\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.060518076321574e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "rel_error = np.abs(diff-diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ReLU and its directional derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def relup(a, e):\n",
    "    '''\n",
    "    The directional derivative of the ReLU function.\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    return (relu(a + eps * e) - relu(a)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Initialization of the net\n",
    "Utilize `He and Xavier initializations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    '''\n",
    "    Inputs:\n",
    "        Ni: dimension of input layer\n",
    "        Nh: dimension of output layer\n",
    "        No: number of unit of output layer\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0] # 784\n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        net: parameters of the net\n",
    "    \n",
    "    Returns:\n",
    "        prediction    \n",
    "    '''\n",
    "    W1 = net[0] # (64, 784)\n",
    "    b1 = net[1] # (64, 1)\n",
    "    W2 = net[2] # (10, 64)\n",
    "    b2 = net[3] # (10, 1)\n",
    "    \n",
    "    a1 = W1.dot(x) + b1 # (64, 60000)\n",
    "    h1 = relu(a1) # (64, 60000)\n",
    "    \n",
    "    a2 = W2.dot(h1) + b2 # (10, 60000)\n",
    "    y = softmax(a2) # (10, 60000)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y, d):\n",
    "    '''\n",
    "    Inputs:\n",
    "        y (10, 60000): prediction\n",
    "        d (10, 60000): ground truth\n",
    "    \n",
    "    Returns:\n",
    "        Average cross-entropy loss (1, 1)\n",
    "    '''\n",
    "    return -np.sum(d*np.log(y))/y.shape[0]/y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2547464657471889 should be around .26\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    '''\n",
    "    Compute the percentage of misclassified samples.\n",
    "    \n",
    "    Inputs:\n",
    "        y (10, 60000): prediction\n",
    "        lbl (10, 60000): ground truth\n",
    "    \n",
    "    Returns:\n",
    "        Error rate\n",
    "    '''\n",
    "    return np.sum(onehot2label(y)!=lbl)/lbl.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9184666666666667\n"
     ]
    }
   ],
   "source": [
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Update the parameters \n",
    "Perform one backpropagation update for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of the net\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # cross-entropy loss\n",
    "    e = -d/y + (1-d)/(1-y)\n",
    "    \n",
    "    # backprop\n",
    "    delta2 = softmaxp(a2, e)\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis=1, keepdims=True)\n",
    "    b1 = b1 - gamma * delta1.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of the net\n",
    "        T: number of updates\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        # COMPLETE TO UPDATE NET\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        # COMPLETE TO DISPLAY LOSS AND PERFS\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        print(f'\\niter = {t+1}')\n",
    "        print(f'loss = {eval_loss(y, d)}')\n",
    "        print(f'error rate = {eval_perfs(y, lbl)}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_max = 100\n",
      "\n",
      "\n",
      "iter = 1\n",
      "loss = 0.2547464657471889\n",
      "error rate = 0.9184666666666667\n",
      "\n",
      "iter = 2\n",
      "loss = 0.22466514643700308\n",
      "error rate = 0.8190333333333333\n",
      "\n",
      "iter = 3\n",
      "loss = 0.2111696595608609\n",
      "error rate = 0.7299333333333333\n",
      "\n",
      "iter = 4\n",
      "loss = 0.2016944092320122\n",
      "error rate = 0.6583166666666667\n",
      "\n",
      "iter = 5\n",
      "loss = 0.19283483716593613\n",
      "error rate = 0.61015\n",
      "\n",
      "iter = 6\n",
      "loss = 0.18527237195204466\n",
      "error rate = 0.5602833333333334\n",
      "\n",
      "iter = 7\n",
      "loss = 0.17773047553957663\n",
      "error rate = 0.5478\n",
      "\n",
      "iter = 8\n",
      "loss = 0.1716846185498394\n",
      "error rate = 0.5100166666666667\n",
      "\n",
      "iter = 9\n",
      "loss = 0.1664138161788615\n",
      "error rate = 0.5240833333333333\n",
      "\n",
      "iter = 10\n",
      "loss = 0.16411071592949647\n",
      "error rate = 0.5234\n",
      "\n",
      "iter = 11\n",
      "loss = 0.16311409915218053\n",
      "error rate = 0.5425166666666666\n",
      "\n",
      "iter = 12\n",
      "loss = 0.16103345862558532\n",
      "error rate = 0.5521666666666667\n",
      "\n",
      "iter = 13\n",
      "loss = 0.15329554880276927\n",
      "error rate = 0.48896666666666666\n",
      "\n",
      "iter = 14\n",
      "loss = 0.14449437840146387\n",
      "error rate = 0.4634666666666667\n",
      "\n",
      "iter = 15\n",
      "loss = 0.13893315405690174\n",
      "error rate = 0.42873333333333336\n",
      "\n",
      "iter = 16\n",
      "loss = 0.13692155306874898\n",
      "error rate = 0.43451666666666666\n",
      "\n",
      "iter = 17\n",
      "loss = 0.1319783627685373\n",
      "error rate = 0.4118\n",
      "\n",
      "iter = 18\n",
      "loss = 0.12782579978601177\n",
      "error rate = 0.3930666666666667\n",
      "\n",
      "iter = 19\n",
      "loss = 0.12061828762004059\n",
      "error rate = 0.36628333333333335\n",
      "\n",
      "iter = 20\n",
      "loss = 0.11747757063262255\n",
      "error rate = 0.33695\n",
      "\n",
      "iter = 21\n",
      "loss = 0.11373647981586975\n",
      "error rate = 0.35015\n",
      "\n",
      "iter = 22\n",
      "loss = 0.11393803878640213\n",
      "error rate = 0.33343333333333336\n",
      "\n",
      "iter = 23\n",
      "loss = 0.10716231307157775\n",
      "error rate = 0.32863333333333333\n",
      "\n",
      "iter = 24\n",
      "loss = 0.11031985888478327\n",
      "error rate = 0.3396\n",
      "\n",
      "iter = 25\n",
      "loss = 0.11090240223118611\n",
      "error rate = 0.34791666666666665\n",
      "\n",
      "iter = 26\n",
      "loss = 0.10484417120666212\n",
      "error rate = 0.3126333333333333\n",
      "\n",
      "iter = 27\n",
      "loss = 0.10255051823785637\n",
      "error rate = 0.3073666666666667\n",
      "\n",
      "iter = 28\n",
      "loss = 0.09107610017788569\n",
      "error rate = 0.26958333333333334\n",
      "\n",
      "iter = 29\n",
      "loss = 0.09372623188443664\n",
      "error rate = 0.28331666666666666\n",
      "\n",
      "iter = 30\n",
      "loss = 0.0921120190093004\n",
      "error rate = 0.29433333333333334\n",
      "\n",
      "iter = 31\n",
      "loss = 0.1006075146070931\n",
      "error rate = 0.30455\n",
      "\n",
      "iter = 32\n",
      "loss = 0.08784383613767288\n",
      "error rate = 0.2879833333333333\n",
      "\n",
      "iter = 33\n",
      "loss = 0.0864568766914456\n",
      "error rate = 0.24473333333333333\n",
      "\n",
      "iter = 34\n",
      "loss = 0.08421253925603406\n",
      "error rate = 0.2734\n",
      "\n",
      "iter = 35\n",
      "loss = 0.08590556445350031\n",
      "error rate = 0.24533333333333332\n",
      "\n",
      "iter = 36\n",
      "loss = 0.08422077960017448\n",
      "error rate = 0.2805666666666667\n",
      "\n",
      "iter = 37\n",
      "loss = 0.08385167952386369\n",
      "error rate = 0.23276666666666668\n",
      "\n",
      "iter = 38\n",
      "loss = 0.07875049523781356\n",
      "error rate = 0.25395\n",
      "\n",
      "iter = 39\n",
      "loss = 0.0773252675445749\n",
      "error rate = 0.2071\n",
      "\n",
      "iter = 40\n",
      "loss = 0.07444213318038505\n",
      "error rate = 0.23335\n",
      "\n",
      "iter = 41\n",
      "loss = 0.07368804048194581\n",
      "error rate = 0.1964\n",
      "\n",
      "iter = 42\n",
      "loss = 0.07166922777653646\n",
      "error rate = 0.22363333333333332\n",
      "\n",
      "iter = 43\n",
      "loss = 0.07067652128146228\n",
      "error rate = 0.19131666666666666\n",
      "\n",
      "iter = 44\n",
      "loss = 0.06991733622247694\n",
      "error rate = 0.2216\n",
      "\n",
      "iter = 45\n",
      "loss = 0.06770945038166498\n",
      "error rate = 0.18491666666666667\n",
      "\n",
      "iter = 46\n",
      "loss = 0.06847367593638759\n",
      "error rate = 0.2195\n",
      "\n",
      "iter = 47\n",
      "loss = 0.06507258905241112\n",
      "error rate = 0.18096666666666666\n",
      "\n",
      "iter = 48\n",
      "loss = 0.06769426775829623\n",
      "error rate = 0.22008333333333333\n",
      "\n",
      "iter = 49\n",
      "loss = 0.06402545111428183\n",
      "error rate = 0.18888333333333332\n",
      "\n",
      "iter = 50\n",
      "loss = 0.06838430123039009\n",
      "error rate = 0.22586666666666666\n",
      "\n",
      "iter = 51\n",
      "loss = 0.06600277249535642\n",
      "error rate = 0.20878333333333332\n",
      "\n",
      "iter = 52\n",
      "loss = 0.06794872981568169\n",
      "error rate = 0.22585\n",
      "\n",
      "iter = 53\n",
      "loss = 0.06679792426902012\n",
      "error rate = 0.21595\n",
      "\n",
      "iter = 54\n",
      "loss = 0.06653336867131777\n",
      "error rate = 0.21918333333333334\n",
      "\n",
      "iter = 55\n",
      "loss = 0.06484328498821194\n",
      "error rate = 0.21066666666666667\n",
      "\n",
      "iter = 56\n",
      "loss = 0.0655709018773352\n",
      "error rate = 0.2113\n",
      "\n",
      "iter = 57\n",
      "loss = 0.061860996372869806\n",
      "error rate = 0.20031666666666667\n",
      "\n",
      "iter = 58\n",
      "loss = 0.06256062133545258\n",
      "error rate = 0.19656666666666667\n",
      "\n",
      "iter = 59\n",
      "loss = 0.05924657380611536\n",
      "error rate = 0.18958333333333333\n",
      "\n",
      "iter = 60\n",
      "loss = 0.06056465734028084\n",
      "error rate = 0.18821666666666667\n",
      "\n",
      "iter = 61\n",
      "loss = 0.05798932736830166\n",
      "error rate = 0.18536666666666668\n",
      "\n",
      "iter = 62\n",
      "loss = 0.05986236855132215\n",
      "error rate = 0.18625\n",
      "\n",
      "iter = 63\n",
      "loss = 0.05784116828221772\n",
      "error rate = 0.18526666666666666\n",
      "\n",
      "iter = 64\n",
      "loss = 0.05952931432959277\n",
      "error rate = 0.18573333333333333\n",
      "\n",
      "iter = 65\n",
      "loss = 0.05806555981978033\n",
      "error rate = 0.18643333333333334\n",
      "\n",
      "iter = 66\n",
      "loss = 0.058708460894630506\n",
      "error rate = 0.18286666666666668\n",
      "\n",
      "iter = 67\n",
      "loss = 0.05771425067252825\n",
      "error rate = 0.18455\n",
      "\n",
      "iter = 68\n",
      "loss = 0.05730305950596408\n",
      "error rate = 0.17706666666666668\n",
      "\n",
      "iter = 69\n",
      "loss = 0.05653678550134044\n",
      "error rate = 0.17906666666666668\n",
      "\n",
      "iter = 70\n",
      "loss = 0.05563474216220249\n",
      "error rate = 0.1701\n",
      "\n",
      "iter = 71\n",
      "loss = 0.054942052029517484\n",
      "error rate = 0.17185\n",
      "\n",
      "iter = 72\n",
      "loss = 0.053981297261139684\n",
      "error rate = 0.16328333333333334\n",
      "\n",
      "iter = 73\n",
      "loss = 0.05326562464696641\n",
      "error rate = 0.16435\n",
      "\n",
      "iter = 74\n",
      "loss = 0.052459839032363786\n",
      "error rate = 0.1573\n",
      "\n",
      "iter = 75\n",
      "loss = 0.05171159282297277\n",
      "error rate = 0.15795\n",
      "\n",
      "iter = 76\n",
      "loss = 0.051157904524139076\n",
      "error rate = 0.1519\n",
      "\n",
      "iter = 77\n",
      "loss = 0.05040148028843815\n",
      "error rate = 0.15301666666666666\n",
      "\n",
      "iter = 78\n",
      "loss = 0.050122842430764\n",
      "error rate = 0.14786666666666667\n",
      "\n",
      "iter = 79\n",
      "loss = 0.04936876268870343\n",
      "error rate = 0.14958333333333335\n",
      "\n",
      "iter = 80\n",
      "loss = 0.04938495920769466\n",
      "error rate = 0.14595\n",
      "\n",
      "iter = 81\n",
      "loss = 0.04862613342476711\n",
      "error rate = 0.14773333333333333\n",
      "\n",
      "iter = 82\n",
      "loss = 0.0489447174785061\n",
      "error rate = 0.14533333333333334\n",
      "\n",
      "iter = 83\n",
      "loss = 0.04820165379245393\n",
      "error rate = 0.14725\n",
      "\n",
      "iter = 84\n",
      "loss = 0.04880144078209655\n",
      "error rate = 0.14665\n",
      "\n",
      "iter = 85\n",
      "loss = 0.04810056914045481\n",
      "error rate = 0.14775\n",
      "\n",
      "iter = 86\n",
      "loss = 0.048832507287275104\n",
      "error rate = 0.14828333333333332\n",
      "\n",
      "iter = 87\n",
      "loss = 0.04818223028759571\n",
      "error rate = 0.14896666666666666\n",
      "\n",
      "iter = 88\n",
      "loss = 0.048758980084147384\n",
      "error rate = 0.14915\n",
      "\n",
      "iter = 89\n",
      "loss = 0.04811858896852156\n",
      "error rate = 0.14836666666666667\n",
      "\n",
      "iter = 90\n",
      "loss = 0.048372066849654054\n",
      "error rate = 0.14761666666666667\n",
      "\n",
      "iter = 91\n",
      "loss = 0.04768843744183364\n",
      "error rate = 0.14638333333333334\n",
      "\n",
      "iter = 92\n",
      "loss = 0.04772342366491435\n",
      "error rate = 0.1451\n",
      "\n",
      "iter = 93\n",
      "loss = 0.04696590282912822\n",
      "error rate = 0.14355\n",
      "\n",
      "iter = 94\n",
      "loss = 0.04695467505778501\n",
      "error rate = 0.14101666666666668\n",
      "\n",
      "iter = 95\n",
      "loss = 0.046159979849540955\n",
      "error rate = 0.13975\n",
      "\n",
      "iter = 96\n",
      "loss = 0.04620109802411345\n",
      "error rate = 0.13776666666666668\n",
      "\n",
      "iter = 97\n",
      "loss = 0.04542177151751957\n",
      "error rate = 0.13673333333333335\n",
      "\n",
      "iter = 98\n",
      "loss = 0.045533605480719413\n",
      "error rate = 0.13506666666666667\n",
      "\n",
      "iter = 99\n",
      "loss = 0.04478713417217073\n",
      "error rate = 0.13341666666666666\n",
      "\n",
      "iter = 100\n",
      "loss = 0.044963874351309274\n",
      "error rate = 0.13276666666666667\n"
     ]
    }
   ],
   "source": [
    "print('\\niter_max = 100\\n')\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.  Evaluate the performance on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest.shape = (784, 10000)\n",
      "ltest.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'xtest.shape = {xtest.shape}')\n",
    "print(f'ltest.shape = {ltest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate = 0.2598\n"
     ]
    }
   ],
   "source": [
    "# use trained network to test testing data\n",
    "pred_test = forwardprop_shallow(xtest, nettrain)\n",
    "print(f'error rate = {eval_perfs(pred_test, ltest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Backpropagation based on minibatch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of net\n",
    "        T: number of epoch\n",
    "        B: minibatches\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of net\n",
    "    '''\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            # COMPLETE TO UPDATE NET\n",
    "            net = update_shallow(x[:,minibatch_indices], d[:,minibatch_indices], net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        # COMPLETE TO DISPLAY LOSS AND PERFS\n",
    "        print(f'\\niter = {t+1}')\n",
    "        print(f'loss = {eval_loss(y, d)}')\n",
    "        print(f'error rate = {eval_perfs(y, lbl)}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter = 1\n",
      "loss = 0.02815563602741705\n",
      "error rate = 0.08113333333333334\n",
      "\n",
      "iter = 2\n",
      "loss = 0.020712688598269982\n",
      "error rate = 0.06015\n",
      "\n",
      "iter = 3\n",
      "loss = 0.016013350631276506\n",
      "error rate = 0.045233333333333334\n",
      "\n",
      "iter = 4\n",
      "loss = 0.013688136894245998\n",
      "error rate = 0.039233333333333335\n",
      "\n",
      "iter = 5\n",
      "loss = 0.012675420382414097\n",
      "error rate = 0.036583333333333336\n"
     ]
    }
   ],
   "source": [
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Evaluate the performance on the testing dataset based on network with minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate = 0.0499\n"
     ]
    }
   ],
   "source": [
    "# use trained network to test testing data\n",
    "pred_test_mini = forwardprop_shallow(xtest, netminibatch)\n",
    "print(f'error rate = {eval_perfs(pred_test_mini, ltest)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
