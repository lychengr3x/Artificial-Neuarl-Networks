{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs) and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify images into ten digits by using an artificial neural network. Let $x \\in \\Re^{784\\times N}$ be a collection of N input images (stacked in columns) and $y \\in [0, 1]^{10\\times N}$ be the collection of our class predictions made for all images and encoded by so-called one-hot codes meant to approximate class probabilities.  \n",
    "\n",
    "A neural network makes the prediction y from the collection x of images by forward propagation which is defined recursively as follows\n",
    "$$\n",
    "\\boldsymbol{y}=\\boldsymbol{h}_L \\quad with \\quad \\boldsymbol{h}_k=g_k(\\boldsymbol{a}_k), \\quad \\boldsymbol{a}_k=\\boldsymbol{W}_k \\boldsymbol{h}_{k-1} + \\boldsymbol{b}_k \\quad and \\quad \\boldsymbol{h}_0=\\boldsymbol{x}\n",
    "$$\n",
    "with $L$ the number of layers, $k$ the layer index, $g_k$ a so-called activation function, $\\boldsymbol{h}_k$ an array of $N$ hidden feature vectors (also stacked in columns), $\\boldsymbol{a}_k$ an array of $N$ vectors of activations (also called potentials), $\\boldsymbol{W}_k$ a matrix of synaptic weights and $\\boldsymbol{b}_k$ a vector of biases.  \n",
    "\n",
    "We will use a shallow neural network which means $L = 2$. The hidden layer will use ReLU activation function and the output layer will use Softmax.  \n",
    "\n",
    "The matrices $\\boldsymbol{W}_k$ and vectors $\\boldsymbol{b}_k$ will be learned/estimated from the MNIST training set by using logistic regression. This consists of minimizing the cross-entropy loss, between the collection d of desired one-hot codes ($d_{ij} = 1$ if the j-th image represents the digit i, 0 otherwise) and the prediction $y$, which is defined as follows\n",
    "$$\n",
    "E=-\\Sigma_{j=1}^{N} \\Sigma_{i=1}^{10} d_{ij} \\log{y_{ij}}\n",
    "$$  \n",
    "\n",
    "The optimization will be performed by gradient descent with backpropagation that can be implemented\n",
    "iteratively, for $t$ = 0, 1, ..., and some initializations $\\boldsymbol{W}_k^0$ and $\\boldsymbol{b}_k^0$ , as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MNISTtools import load, show\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ltrain = load(dataset='training', path='dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain.shape = (784, 60000)\n",
      "ltrain.shape = (60000,)\n",
      "size of training set = 60000\n",
      "feature dimension = 784\n"
     ]
    }
   ],
   "source": [
    "print(f'xtrain.shape = {xtrain.shape}')\n",
    "print(f'ltrain.shape = {ltrain.shape}')\n",
    "print(f'size of training set = {xtrain.shape[1]}')\n",
    "print(f'feature dimension = {xtrain.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display the image of index 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label of index 42 is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'label of index 42 is {ltrain[42]}')\n",
    "show(xtrain[:,42].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The range and type of `xtrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of xtrain = from 0 to 255\n",
      "type of xtrain = <class 'numpy.ndarray'>\n",
      "dtype of xtrain = uint8\n"
     ]
    }
   ],
   "source": [
    "print(f'range of xtrain = from {xtrain.min()} to {xtrain.max()}')\n",
    "print(f'type of xtrain = {type(xtrain)}')\n",
    "print(f'dtype of xtrain = {xtrain.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize `xtrain`\n",
    "Normalize `xtrain` s.t. it is in the range `[-1,1]` of type `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "    '''\n",
    "    x_norm = x.astype(np.float32)\n",
    "    return x_norm*2/255-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min normalized xtrain = -1.0\n",
      "max normalized xtrain = 1.0\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print(f'min normalized xtrain = {np.min(xtrain)}')\n",
    "print(f'max normalized xtrain = {np.max(xtrain)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    '''\n",
    "    Convert label (n,) to one-hot form (d,n).\n",
    "    '''\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtrain shape = (10, 60000)\n",
      "(array([7]),)\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(f'dtrain shape = {dtrain.shape}')\n",
    "print(f'{np.where(dtrain[:,42] == 1)}')\n",
    "print(f'{np.where(dtrain[:,42] == 1) == ltrain[42]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. One-hot decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    '''\n",
    "    Inputs:\n",
    "        d: one-hot encoding labels\n",
    "    '''\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(ltrain == onehot2label(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The softmax function\n",
    "$$\n",
    "y_i=g(\\boldsymbol{a})_i=\\frac{e^{a_i}}{\\Sigma_{j=1}^{10} e^{a_j}} \\quad where \\quad i\\in[1,10]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    '''\n",
    "    Inputs: \n",
    "        a: data\n",
    "    '''\n",
    "    exp_a = np.exp(a - np.max(a, axis=0))\n",
    "    return exp_a / np.sum(exp_a, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_i} = {g(\\boldsymbol{a})}_i (1-{g(\\boldsymbol{a})_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_i} & = \n",
    "  \\frac{\\partial}{\\partial a_i}\\frac{e^{a_i}}{\\Sigma_{k=1}^{N} e^{a_k}} \\\\\n",
    "& = \\frac{e^{a_i}\\Sigma_{k=1}^{N}e^{a_k}-e^{a_i}e^{a_i}}{(\\Sigma_{k=1}^{N}e^{a_k})^2} \\\\\n",
    "& = \\frac{e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\frac{\\Sigma_{k=1}^{N}e^{a_k}-e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\\\\n",
    "& = {g(\\boldsymbol{a})}_i (1-{g(\\boldsymbol{a})_i})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{g(\\boldsymbol{a})_i}}{\\partial a_j}=-g(\\boldsymbol{a})_i g(\\boldsymbol{a})_j \\quad for \\, j \\neq i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial g(\\boldsymbol{a})_i}{\\partial a_j} & = \n",
    "  \\frac{\\partial}{\\partial a_j} \\frac{e^{a_i}}{\\Sigma_{k=1}^{N} e^{a_k}} \\\\\n",
    "& = \\frac{0-e^{a_i}e^{a_j}}{(\\Sigma_{k=1}^{N}e^{a_k})^2} \\\\\n",
    "& = -\\frac{e^{a_i}}{\\Sigma_{k=1}^{N}e^{a_k}} \\frac{e^{a_j}}{\\Sigma_{k=1}^{N}e^{a_k}} \\\\\n",
    "& = -{g(\\boldsymbol{a})}_i g(\\boldsymbol{a})_j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute the $\\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta = g(a)\\otimes e-<g(a),e>g(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    '''\n",
    "    Compute delta for the backpropagation.\n",
    "    \n",
    "    Inputs:\n",
    "        a: predictions before softmax\n",
    "        e: onehot labels\n",
    "        \n",
    "    Returns:\n",
    "        delta\n",
    "    '''\n",
    "    softmax_a = softmax(a)\n",
    "    \n",
    "    # element-wise product\n",
    "    e_prod = softmax_a*e\n",
    "    \n",
    "    # dot product\n",
    "    dot_prod = np.sum(e_prod, axis=0)\n",
    "    \n",
    "    return e_prod - dot_prod*softmax_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Numerical approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta=\\frac{\\partial g(a)}{\\partial a}\\times e = \\lim_{\\epsilon \\to 0}\\frac{g(a+\\epsilon e)-g(a)}{\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.094179078676417e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "rel_error = np.abs(diff-diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ReLU and its directional derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def relup(a, e):\n",
    "    '''\n",
    "    The directional derivative of the ReLU function.\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    return (relu(a + eps * e) - relu(a)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "\n",
    "diff = relup(a, e)\n",
    "diff_approx = (relu(a + eps*e) - relu(a)) / eps\n",
    "rel_error = np.abs(diff-diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Initialization of the net\n",
    "Utilize `He and Xavier initializations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    '''\n",
    "    Inputs:\n",
    "        Ni: dimension of input layer\n",
    "        Nh: dimension of output layer\n",
    "        No: number of unit of output layer\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0] # 784\n",
    "Nh = 64\n",
    "No = dtrain.shape[0] # 10\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        net: parameters of the net\n",
    "    \n",
    "    Returns:\n",
    "        prediction    \n",
    "    '''\n",
    "    W1 = net[0] # (64, 784)\n",
    "    b1 = net[1] # (64, 1)\n",
    "    W2 = net[2] # (10, 64)\n",
    "    b2 = net[3] # (10, 1)\n",
    "    \n",
    "    a1 = W1.dot(x) + b1 # (64, 60000)\n",
    "    h1 = relu(a1) # (64, 60000)\n",
    "    \n",
    "    a2 = W2.dot(h1) + b2 # (10, 60000)\n",
    "    y = softmax(a2) # (10, 60000)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y, d):\n",
    "    '''\n",
    "    Inputs:\n",
    "        y (10, 60000): prediction\n",
    "        d (10, 60000): ground truth\n",
    "    \n",
    "    Returns:\n",
    "        Average cross-entropy loss (1, 1)\n",
    "    '''\n",
    "    return -np.sum(d*np.log(y))/y.shape[0]/y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2590637063157346 should be around .26\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    '''\n",
    "    Compute the percentage of misclassified samples.\n",
    "    \n",
    "    Inputs:\n",
    "        y (10, 60000): prediction\n",
    "        lbl (10, 60000): ground truth\n",
    "    \n",
    "    Returns:\n",
    "        Error rate\n",
    "    '''\n",
    "    return np.sum(onehot2label(y)!=lbl)/lbl.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9048\n"
     ]
    }
   ],
   "source": [
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Update the parameters \n",
    "Perform one backpropagation update for the network.\n",
    "\n",
    "$$\n",
    "E=-\\Sigma_{j=1}^{N} \\Sigma_{i=1}^{10} d_{ij} \\log{y_{ij}} \\\\\n",
    "(\\nabla_y E)_i=-\\frac{d_i}{y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of the net\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # derivative of cross-entropy loss\n",
    "    e = -d/y + (1-d)/(1-y)\n",
    "    \n",
    "    # backprop\n",
    "    delta2 = softmaxp(a2, e)\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis=1, keepdims=True)\n",
    "    b1 = b1 - gamma * delta1.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of the net\n",
    "        T: number of updates\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of the net\n",
    "    '''\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        # UPDATE NET\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        # DISPLAY LOSS AND PERFS\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        print(f'\\niter = {t+1}')\n",
    "        print(f'loss = {eval_loss(y, d):.4f}')\n",
    "        print(f'error rate = {eval_perfs(y, lbl):.4f}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter_max = 100\n",
      "\n",
      "\n",
      "iter = 1\n",
      "loss = 0.2591\n",
      "error rate = 0.9048\n",
      "\n",
      "iter = 2\n",
      "loss = 0.2342\n",
      "error rate = 0.9045\n",
      "\n",
      "iter = 3\n",
      "loss = 0.2183\n",
      "error rate = 0.7650\n",
      "\n",
      "iter = 4\n",
      "loss = 0.2080\n",
      "error rate = 0.6876\n",
      "\n",
      "iter = 5\n",
      "loss = 0.1998\n",
      "error rate = 0.6366\n",
      "\n",
      "iter = 6\n",
      "loss = 0.1915\n",
      "error rate = 0.5912\n",
      "\n",
      "iter = 7\n",
      "loss = 0.1847\n",
      "error rate = 0.5589\n",
      "\n",
      "iter = 8\n",
      "loss = 0.1778\n",
      "error rate = 0.5525\n",
      "\n",
      "iter = 9\n",
      "loss = 0.1774\n",
      "error rate = 0.5410\n",
      "\n",
      "iter = 10\n",
      "loss = 0.1750\n",
      "error rate = 0.5970\n",
      "\n",
      "iter = 11\n",
      "loss = 0.1878\n",
      "error rate = 0.5616\n",
      "\n",
      "iter = 12\n",
      "loss = 0.1619\n",
      "error rate = 0.4986\n",
      "\n",
      "iter = 13\n",
      "loss = 0.1540\n",
      "error rate = 0.4714\n",
      "\n",
      "iter = 14\n",
      "loss = 0.1465\n",
      "error rate = 0.4331\n",
      "\n",
      "iter = 15\n",
      "loss = 0.1485\n",
      "error rate = 0.4530\n",
      "\n",
      "iter = 16\n",
      "loss = 0.1434\n",
      "error rate = 0.4373\n",
      "\n",
      "iter = 17\n",
      "loss = 0.1461\n",
      "error rate = 0.4387\n",
      "\n",
      "iter = 18\n",
      "loss = 0.1313\n",
      "error rate = 0.3800\n",
      "\n",
      "iter = 19\n",
      "loss = 0.1306\n",
      "error rate = 0.3849\n",
      "\n",
      "iter = 20\n",
      "loss = 0.1206\n",
      "error rate = 0.3457\n",
      "\n",
      "iter = 21\n",
      "loss = 0.1260\n",
      "error rate = 0.3752\n",
      "\n",
      "iter = 22\n",
      "loss = 0.1165\n",
      "error rate = 0.3598\n",
      "\n",
      "iter = 23\n",
      "loss = 0.1245\n",
      "error rate = 0.3843\n",
      "\n",
      "iter = 24\n",
      "loss = 0.1219\n",
      "error rate = 0.4135\n",
      "\n",
      "iter = 25\n",
      "loss = 0.1139\n",
      "error rate = 0.3507\n",
      "\n",
      "iter = 26\n",
      "loss = 0.1150\n",
      "error rate = 0.3901\n",
      "\n",
      "iter = 27\n",
      "loss = 0.1070\n",
      "error rate = 0.3253\n",
      "\n",
      "iter = 28\n",
      "loss = 0.1097\n",
      "error rate = 0.3665\n",
      "\n",
      "iter = 29\n",
      "loss = 0.0972\n",
      "error rate = 0.2769\n",
      "\n",
      "iter = 30\n",
      "loss = 0.0947\n",
      "error rate = 0.2929\n",
      "\n",
      "iter = 31\n",
      "loss = 0.0914\n",
      "error rate = 0.2713\n",
      "\n",
      "iter = 32\n",
      "loss = 0.0923\n",
      "error rate = 0.2891\n",
      "\n",
      "iter = 33\n",
      "loss = 0.0863\n",
      "error rate = 0.2505\n",
      "\n",
      "iter = 34\n",
      "loss = 0.0848\n",
      "error rate = 0.2551\n",
      "\n",
      "iter = 35\n",
      "loss = 0.0827\n",
      "error rate = 0.2475\n",
      "\n",
      "iter = 36\n",
      "loss = 0.0820\n",
      "error rate = 0.2456\n",
      "\n",
      "iter = 37\n",
      "loss = 0.0797\n",
      "error rate = 0.2373\n",
      "\n",
      "iter = 38\n",
      "loss = 0.0776\n",
      "error rate = 0.2281\n",
      "\n",
      "iter = 39\n",
      "loss = 0.0768\n",
      "error rate = 0.2307\n",
      "\n",
      "iter = 40\n",
      "loss = 0.0745\n",
      "error rate = 0.2162\n",
      "\n",
      "iter = 41\n",
      "loss = 0.0741\n",
      "error rate = 0.2211\n",
      "\n",
      "iter = 42\n",
      "loss = 0.0715\n",
      "error rate = 0.2047\n",
      "\n",
      "iter = 43\n",
      "loss = 0.0714\n",
      "error rate = 0.2128\n",
      "\n",
      "iter = 44\n",
      "loss = 0.0687\n",
      "error rate = 0.1951\n",
      "\n",
      "iter = 45\n",
      "loss = 0.0690\n",
      "error rate = 0.2062\n",
      "\n",
      "iter = 46\n",
      "loss = 0.0661\n",
      "error rate = 0.1886\n",
      "\n",
      "iter = 47\n",
      "loss = 0.0679\n",
      "error rate = 0.2090\n",
      "\n",
      "iter = 48\n",
      "loss = 0.0653\n",
      "error rate = 0.2019\n",
      "\n",
      "iter = 49\n",
      "loss = 0.0720\n",
      "error rate = 0.2350\n",
      "\n",
      "iter = 50\n",
      "loss = 0.0752\n",
      "error rate = 0.2279\n",
      "\n",
      "iter = 51\n",
      "loss = 0.0654\n",
      "error rate = 0.2027\n",
      "\n",
      "iter = 52\n",
      "loss = 0.0629\n",
      "error rate = 0.1966\n",
      "\n",
      "iter = 53\n",
      "loss = 0.0655\n",
      "error rate = 0.2115\n",
      "\n",
      "iter = 54\n",
      "loss = 0.0706\n",
      "error rate = 0.2205\n",
      "\n",
      "iter = 55\n",
      "loss = 0.0606\n",
      "error rate = 0.1827\n",
      "\n",
      "iter = 56\n",
      "loss = 0.0593\n",
      "error rate = 0.1822\n",
      "\n",
      "iter = 57\n",
      "loss = 0.0596\n",
      "error rate = 0.1853\n",
      "\n",
      "iter = 58\n",
      "loss = 0.0637\n",
      "error rate = 0.2066\n",
      "\n",
      "iter = 59\n",
      "loss = 0.0576\n",
      "error rate = 0.1756\n",
      "\n",
      "iter = 60\n",
      "loss = 0.0590\n",
      "error rate = 0.1892\n",
      "\n",
      "iter = 61\n",
      "loss = 0.0568\n",
      "error rate = 0.1752\n",
      "\n",
      "iter = 62\n",
      "loss = 0.0601\n",
      "error rate = 0.1966\n",
      "\n",
      "iter = 63\n",
      "loss = 0.0556\n",
      "error rate = 0.1693\n",
      "\n",
      "iter = 64\n",
      "loss = 0.0577\n",
      "error rate = 0.1866\n",
      "\n",
      "iter = 65\n",
      "loss = 0.0549\n",
      "error rate = 0.1684\n",
      "\n",
      "iter = 66\n",
      "loss = 0.0576\n",
      "error rate = 0.1877\n",
      "\n",
      "iter = 67\n",
      "loss = 0.0540\n",
      "error rate = 0.1654\n",
      "\n",
      "iter = 68\n",
      "loss = 0.0561\n",
      "error rate = 0.1808\n",
      "\n",
      "iter = 69\n",
      "loss = 0.0531\n",
      "error rate = 0.1623\n",
      "\n",
      "iter = 70\n",
      "loss = 0.0551\n",
      "error rate = 0.1764\n",
      "\n",
      "iter = 71\n",
      "loss = 0.0521\n",
      "error rate = 0.1587\n",
      "\n",
      "iter = 72\n",
      "loss = 0.0538\n",
      "error rate = 0.1703\n",
      "\n",
      "iter = 73\n",
      "loss = 0.0512\n",
      "error rate = 0.1554\n",
      "\n",
      "iter = 74\n",
      "loss = 0.0527\n",
      "error rate = 0.1654\n",
      "\n",
      "iter = 75\n",
      "loss = 0.0503\n",
      "error rate = 0.1524\n",
      "\n",
      "iter = 76\n",
      "loss = 0.0517\n",
      "error rate = 0.1609\n",
      "\n",
      "iter = 77\n",
      "loss = 0.0495\n",
      "error rate = 0.1505\n",
      "\n",
      "iter = 78\n",
      "loss = 0.0510\n",
      "error rate = 0.1585\n",
      "\n",
      "iter = 79\n",
      "loss = 0.0490\n",
      "error rate = 0.1492\n",
      "\n",
      "iter = 80\n",
      "loss = 0.0507\n",
      "error rate = 0.1569\n",
      "\n",
      "iter = 81\n",
      "loss = 0.0489\n",
      "error rate = 0.1500\n",
      "\n",
      "iter = 82\n",
      "loss = 0.0506\n",
      "error rate = 0.1573\n",
      "\n",
      "iter = 83\n",
      "loss = 0.0492\n",
      "error rate = 0.1531\n",
      "\n",
      "iter = 84\n",
      "loss = 0.0506\n",
      "error rate = 0.1575\n",
      "\n",
      "iter = 85\n",
      "loss = 0.0495\n",
      "error rate = 0.1556\n",
      "\n",
      "iter = 86\n",
      "loss = 0.0503\n",
      "error rate = 0.1566\n",
      "\n",
      "iter = 87\n",
      "loss = 0.0495\n",
      "error rate = 0.1561\n",
      "\n",
      "iter = 88\n",
      "loss = 0.0497\n",
      "error rate = 0.1526\n",
      "\n",
      "iter = 89\n",
      "loss = 0.0489\n",
      "error rate = 0.1537\n",
      "\n",
      "iter = 90\n",
      "loss = 0.0487\n",
      "error rate = 0.1478\n",
      "\n",
      "iter = 91\n",
      "loss = 0.0480\n",
      "error rate = 0.1495\n",
      "\n",
      "iter = 92\n",
      "loss = 0.0477\n",
      "error rate = 0.1425\n",
      "\n",
      "iter = 93\n",
      "loss = 0.0470\n",
      "error rate = 0.1451\n",
      "\n",
      "iter = 94\n",
      "loss = 0.0467\n",
      "error rate = 0.1383\n",
      "\n",
      "iter = 95\n",
      "loss = 0.0461\n",
      "error rate = 0.1416\n",
      "\n",
      "iter = 96\n",
      "loss = 0.0459\n",
      "error rate = 0.1343\n",
      "\n",
      "iter = 97\n",
      "loss = 0.0452\n",
      "error rate = 0.1375\n",
      "\n",
      "iter = 98\n",
      "loss = 0.0452\n",
      "error rate = 0.1318\n",
      "\n",
      "iter = 99\n",
      "loss = 0.0446\n",
      "error rate = 0.1347\n",
      "\n",
      "iter = 100\n",
      "loss = 0.0446\n",
      "error rate = 0.1298\n"
     ]
    }
   ],
   "source": [
    "print('\\niter_max = 100\\n')\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.  Evaluate the performance on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest.shape = (784, 10000)\n",
      "ltest.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = load(dataset='testing', path='dataset/')\n",
    "print(f'xtest.shape = {xtest.shape}')\n",
    "print(f'ltest.shape = {ltest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate = 0.3709\n"
     ]
    }
   ],
   "source": [
    "# use trained network to evaluate the performance on testing data\n",
    "pred_test = forwardprop_shallow(xtest, nettrain)\n",
    "print(f'error rate = {eval_perfs(pred_test, ltest):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Backpropagation based on minibatch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: data\n",
    "        d: ground truth\n",
    "        net: parameters of net\n",
    "        T: number of epoch\n",
    "        B: minibatches\n",
    "        gamma: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        parameters of net\n",
    "    '''\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T): # epoch\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            # UPDATE NET\n",
    "            net = update_shallow(x[:,minibatch_indices], d[:,minibatch_indices], net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        # DISPLAY LOSS AND PERFS\n",
    "        print(f'\\nepoch = {t+1}')\n",
    "        print(f'loss = {eval_loss(y, d):.4f}')\n",
    "        print(f'error rate = {eval_perfs(y, lbl):.4f}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Evaluate the performance on the testing dataset based on network with minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch = 1\n",
      "loss = 0.0282\n",
      "error rate = 0.0834\n",
      "\n",
      "epoch = 2\n",
      "loss = 0.0203\n",
      "error rate = 0.0577\n",
      "\n",
      "epoch = 3\n",
      "loss = 0.0177\n",
      "error rate = 0.0503\n",
      "\n",
      "epoch = 4\n",
      "loss = 0.0143\n",
      "error rate = 0.0409\n",
      "\n",
      "epoch = 5\n",
      "loss = 0.0138\n",
      "error rate = 0.0404\n"
     ]
    }
   ],
   "source": [
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate = 0.0612\n"
     ]
    }
   ],
   "source": [
    "# use trained network to evaluate the performance on testing data\n",
    "pred_test_mini = forwardprop_shallow(xtest, netminibatch)\n",
    "print(f'error rate = {eval_perfs(pred_test_mini, ltest):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
